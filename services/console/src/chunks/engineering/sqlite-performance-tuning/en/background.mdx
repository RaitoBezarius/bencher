## Background

From the very start, I knew that the [Bencher Perf API][perf query]
was going to be one of the most demanding endpoints performance wise.
I believe the main reason that so many folks have had to [reinvent the benchmark tracking wheel][prior art]
is that the existing off-the-shelf tools don't handle the high dimensionality required.
By "high dimensionality", I mean being able to track performance over time and across multiple dimensions:
[Branches][branch], [Testbeds][testbed], [Benchmarks][benchmarks], and [Measures][measures].
This ability to slice and dice across five different dimensions leads to a very complex model.

Because of this inherent complexity and the nature of the data,
I considered using a time series database for Bencher.
In the end though, I settled on using SQLite instead.
I figured it was better to [do things that don't scale][do things that dont scale]
than to spend the extra time learning an entirely new database architecture that may or may not actually help.

Over time, the demands on the Bencher Perf API have also increased.
Originally, you had to select all of the dimensions that you wanted to plot manually.
This created a lot of friction for users to get to a useful plot.
To solve this, I [added a list of the most recent Reports][github issue 133] to the Perf Pages,
and by default, the most recent Report was selected and plotted.
This means that if there were 112 benchmarks in the most recent Report, then all 112 would be plotted.
The model also got even more complicated with the ability to track and visualize [Threshold Boundaries][thresholds].

With this in mind, I made a few performance related improvements.
Since the Perf Plot needs the most recent Report to start plotting,
I refactored the [Reports API][reports api] to get a Report's result data in a single call to the database instead of iterating.
The time window for the default Report query was set to four weeks, instead of being unbounded.
I also drastically limited the scope of all database handles, reducing lock contention.
To help communicate to users, I added a status bar spinner for both [the Perf Plot][bencher v0317] and [the dimension tabs][bencher v045].

I also had a failed attempt last fall at using a composite query to get all Perf results into a single query,
instead of using a quadruple nested for loop.
This lead to me hitting the [Rust type system recursion limit][recusion limit],
repeatedly overflowing the stack,
suffering through insane (much longer than 38 seconds) compile times,
and finally dead ending at [SQLite's max number of terms in a compound select statement][sqlite limits].

With all of that under my belt, I knew that I really needed to dig in here
and put my performance engineer pants on.
I had never profiled a SQLite database before,
and honestly, I had never really profiled _any_ database before.
Now wait a minute you might might be thinking.
[My LinkedIn profile][linkedin epompeii] says I was a "Database Administrator" for almost two years.
And I _never_ profiled a databaseâ€½
Yep. That's a story for another time I suppose.

[do things that dont scale]: https://paulgraham.com/ds.html
[github issue 133]: https://github.com/bencherdev/bencher/issues/133
[recusion limit]: https://doc.rust-lang.org/reference/attributes/limits.html#the-recursion_limit-attribute
[sqlite limits]: https://www.sqlite.org/limits.html
[linkedin epompeii]: https://www.linkedin.com/in/epompeii/

[perf query]: /docs/api/projects/perf/#get-v0projectsprojectperf
[prior art]: /docs/reference/prior-art/#benchmark-tracking-tools
[branch]: /docs/explanation/benchmarking/#branch
[testbed]: /docs/explanation/benchmarking/#testbed
[benchmarks]: /docs/explanation/benchmarking/#benchmark
[measures]: /docs/explanation/benchmarking/#measure
[thresholds]: /docs/explanation/thresholds/
[reports api]: /docs/api/projects/reports/#get-v0projectsprojectreports
[bencher v0317]: /docs/reference/changelog/#v0317
[bencher v045]: /docs/reference/changelog/#v045

import QSMockOutput from "../qs-mock-output.mdx";
import QSRunOutput from "../qs-run-output.mdx";
import QSExportProjectOutput from "../qs-export-project-output.mdx";

## Ex√©cutez un Rapport

Nous sommes enfin pr√™ts √† collecter quelques mesures de benchmark!
Pour des raisons de simplicit√©, nous utiliserons des donn√©es fictives dans ce tutoriel.

Ex√©cutez: `bencher mock`

Vous devriez voir quelque chose comme:

<QSMockOutput />

Votre sortie devrait √™tre l√©g√®rement diff√©rente de celle-ci, car les donn√©es sont pseudorandom.
Il est juste important que cette commande fonctionne.

<br />

Maintenant, ex√©cutons un rapport en utilisant des donn√©es fictives de mesures de benchmark.
Substituez votre `slug` de Projet √† l'argument `--project` (ie `YOUR_PROJECT_SLUG`) dans la commande ci-dessous.

Ex√©cutez: `bencher run --project YOUR_PROJECT_SLUG "bencher mock"`

Vous devriez voir quelque chose comme:

<QSRunOutput />

<br />

Vous pouvez maintenant consulter les r√©sultats de chacun des benchmarks dans le navigateur.
Cliquez ou copiez et collez les liens de `View results`.
Il ne devrait y avoir qu'un seul point de donn√©es pour chaque benchmark, alors ajoutons d'autres donn√©es!

<br />

D'abord, mettons notre slug de Projet en tant que variable d'environnement, de sorte que nous n'avons pas √† le fournir avec le `--project` √† chaque ex√©cution.

Ex√©cutez: `export BENCHER_PROJECT=save-walter-white-1234abcd`

Si vous ex√©cutez ensuite: `echo $BENCHER_PROJECT`

Vous devriez voir:

<QSExportProjectOutput />

<br />

R√©-ex√©cutons la m√™me commande encore sans `--project` pour g√©n√©rer plus de donn√©es.

Ex√©cutez: `bencher run "bencher mock"`

<br />

Maintenant, g√©n√©rions plus de donn√©es, mais cette fois nous transmettrons nos r√©sultats dans `bencher run`.

Ex√©cutez: `bencher mock | bencher run`

<br />

Parfois, vous pouvez vouloir sauvegarder vos r√©sultats dans un fichier et que `bencher run` les r√©cup√®re.

Ex√©cutez: `bencher run --file results.json "bencher mock > results.json"`

<br />

De m√™me, vous pouvez avoir un processus s√©par√© qui ex√©cute vos benchmarks et sauvegardez vos r√©sultats dans un fichier. Ensuite `bencher run` viendra simplement les r√©cup√©rer.

Ex√©cutez: `bencher mock > results.json && bencher run --file results.json`

<br />

Enfin, mettons en place beaucoup de donn√©es en utilisant l'argument `--iter` de `bencher run`.

Ex√©cutez: `bencher run --iter 16 "bencher mock"`

<br />

> üê∞ Astuce: Consultez les [doc CLI sous-commande `bencher run`](/fr/docs/explanation/bencher-run/) pour un aper√ßu complet de tout ce que `bencher run` peut faire!
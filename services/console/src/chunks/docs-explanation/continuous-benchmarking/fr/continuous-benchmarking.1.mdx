Le Benchmarking Continu est une pratique de d√©veloppement logiciel o√π les membres d'une √©quipe √©valuent leur travail fr√©quemment,
en g√©n√©ral, chaque personne fait cela au moins une fois par jour - ce qui conduit √† plusieurs benchmarks par jour.
Chaque benchmark est v√©rifi√© par une construction automatis√©e pour d√©tecter les r√©gressions de performances le plus rapidement possible.
De nombreuses √©quipes trouvent que cette approche conduit √† des r√©gressions de performances nettement r√©duites
et permet √† une √©quipe de d√©velopper des logiciels performants plus rapidement.

√Ä pr√©sent, tout le monde dans l'industrie du logiciel est au courant de l'int√©gration continue (CI).
Au niveau fondamental, la CI consiste √† d√©tecter et √† pr√©venir les r√©gressions de fonctionnalit√©s logicielles avant qu'elles n'arrivent en production.
De la m√™me mani√®re, le benchmarking continu (CB) consiste √† d√©tecter et √† pr√©venir les r√©gressions de _performance_ logicielle avant qu'elles n'arrivent en production.
Pour les m√™mes raisons que les tests unitaires sont ex√©cut√©s en CI pour chaque changement de code,
des tests de performance devraient √™tre ex√©cut√©s en CB pour chaque changement de code.
Cette analogie est si pertinente en fait, que le premier paragraphe de cette section n'est qu'une version Mad Libs [de l'introduction de Martin Fowler √† l'int√©gration continue en 2006](https://martinfowler.com/articles/continuousIntegration.html).

> üê∞ Les bugs de performance sont des bugs !

## Benchmarking en CI

Mythe : Vous ne pouvez pas ex√©cuter de benchmarks en CI

La plupart des harnais de benchmarking utilisent [l'horloge murale du syst√®me](https://en.wikipedia.org/wiki/Elapsed_real_time) pour mesurer la latence ou le d√©bit.
Ceci est tr√®s utile, car ce sont pr√©cis√©ment ces param√®tres qui nous concernent le plus en tant que d√©veloppeurs.
Cependant, les environnements de CI √† usage g√©n√©ral sont souvent bruyants et inconsistants lors de la mesure du temps d'horloge murale.
Lors du benchmarking continu, cette volatilit√© ajoute un bruit ind√©sirable dans les r√©sultats.

Il existe plusieurs options pour g√©rer cela :
- [Benchmarking relatif](/fr/docs/how-to/track-benchmarks/)
- Des ex√©cutants de CI d√©di√©s
- Changer de harnais de benchmarking pour l'un qui compte les instructions par opposition au temps d'horloge murale

Ou simplement embrasser le chaos¬†! Le benchmarking continu n'a pas besoin d'√™tre parfait.
Oui, r√©duire la volatilit√© et donc le bruit dans votre environnement de benchmarking continu vous permettra de d√©tecter des r√©gressions de performance de plus en plus fines.
Cependant, ne laissez pas la perfection √™tre l'ennemie du bien ici !

<div style="text-align: center;">
<a href="https://bencher.dev/perf/bencher?key=true&measures=4358146b-b647-4869-9d24-bd22bb0c49b5&branches=619d15ed-0fbd-4ccb-86cb-fddf3124da29&tab=benchmarks&testbeds=0d991aac-b241-493a-8b0f-8d41419455d2&benchmarks=1db23e93-f909-40aa-bf42-838cc7ae05f5"><img style="border: 0.2em solid #ed6704;" src="https://api.bencher.dev/v0/projects/bencher/perf/img?branches=619d15ed-0fbd-4ccb-86cb-fddf3124da29&testbeds=0d991aac-b241-493a-8b0f-8d41419455d2&benchmarks=1db23e93-f909-40aa-bf42-838cc7ae05f5&measures=4358146b-b647-4869-9d24-bd22bb0c49b5&title=Embrace+the+Chaos%21" title="Embrace the Chaos!" alt="Embrace the Chaos! for Bencher - Bencher" width="1024" height="768" /></a>
</div>

Vous pourriez regarder ce graphique et penser, "Waouh, c'est fou !" Mais demandez-vous, votre processus de d√©veloppement actuel peut-il d√©tecter une r√©gression de performance de deux ou m√™me dix fois avant qu'elle n'affecte vos utilisateurs ? Probablement pas ! Maintenant, _√ßa_, c'est fou !

M√™me avec tout le bruit d'un environnement de CI, le suivi des benchmarks d'horloge murale peut encore rapporter de grands b√©n√©fices en d√©tectant les r√©gressions de performance avant qu'elles n'atteignent vos clients en production.
Avec le temps, √† mesure que la gestion des performances de votre logiciel m√ªrit, vous pourrez vous am√©liorer √† partir de l√†.
En attendant, utilisez simplement votre CI habituelle.

## La Performance Compte

Mythe : Vous ne pouvez pas remarquer 100ms de latence

Il est courant d'entendre des gens affirmer que les humains ne peuvent percevoir 100ms de latence.
Un [article du groupe Nielsen sur les temps de r√©ponse](https://www.nngroup.com/articles/response-times-3-important-limits/) est souvent cit√© pour cette affirmation.

> **0,1 seconde** est √† peu pr√®s la limite pour faire en sorte que l'utilisateur ait l'impression que le syst√®me r√©agit **instantan√©ment**, ce qui signifie qu'aucun retour sp√©cial n'est n√©cessaire except√© pour afficher le r√©sultat.
>
> - Jakob Nielsen, 1 Jan __*1993*__

Mais ce n'est tout simplement pas vrai.
Sur certaines t√¢ches, les gens peuvent percevoir [aussi peu que 2ms de latence](https://pdfs.semanticscholar.org/386a/15fd85c162b8e4ebb6023acdce9df2bd43ee.pdf).
Un moyen facile de le prouver est une [exp√©rience de Dan Luu](https://danluu.com/input-lag/#appendix-why-measure-latency): ouvrez votre terminal et ex√©cutez `sleep 0; echo "ping"` puis ex√©cutez `sleep 0.1; echo "pong"`. Vous avez remarqu√© la diff√©rence, n'est-ce pas‚ÄΩ

Un autre point commun de confusion est la distinction entre la perception de la latence et les temps de r√©action humains. M√™me s'il faut [environ 200ms pour r√©pondre √† un stimulus visuel](https://humanbenchmark.com/tests/reactiontime), cela est ind√©pendant de la perception de l'√©v√©nement lui-m√™me. Par analogie, vous pouvez remarquer que votre train a deux minutes de retard (latence per√ßue) m√™me si le trajet en train dure deux heures (temps de r√©action).

La performance compte¬†! [La performance est une caract√©ristique](https://blog.codinghorror.com/performance-is-a-feature) !

- Chaque 100ms plus rapide ‚Üí 1% de conversions en plus ([Mobify](https://web.dev/why-speed-matters/), gagnant +380 000$/an)
- 50% plus rapide ‚Üí 12% de ventes en plus ([AutoAnything](https://www.digitalcommerce360.com/2010/08/19/web-accelerator-revs-conversion-and-sales-autoanything/))
- 20% plus rapide ‚Üí 10% de conversions en plus ([Furniture Village](https://www.thinkwithgoogle.com/intl/en-gb/marketing-strategies/app-and-mobile/furniture-village-and-greenlight-slash-page-load-times-boosting-user-experience/))
- 40% plus rapide ‚Üí 15% de plus d'inscriptions ([Pinterest](https://medium.com/pinterest-engineering/driving-user-growth-with-performance-improvements-cfc50dafadd7))
- 850ms plus rapide ‚Üí 7% de conversions en plus ([COOK](https://web.dev/why-speed-matters/))
- Chaque seconde plus lent ‚Üí 10% d'utilisateurs en moins ([BBC](https://www.creativebloq.com/features/how-the-bbc-builds-websites-that-scale))

Avec la fin de la loi de Moore, les charges de travail qui peuvent s'ex√©cuter en parall√®le devront √™tre parall√©lis√©es.
Cependant, la plupart des charges de travail doivent s'ex√©cuter en s√©rie,
et le fait de simplement jeter plus de calcul sur le probl√®me devient rapidement une solution intractable et co√ªteuse.

Le Benchmarking Continu est un √©l√©ment cl√© pour d√©velopper et maintenir
des logiciels modernes performants face √† ce changement.

<div class="content has-text-centered">
<img
    src="https://s3.amazonaws.com/public.bencher.dev/docs/moores_law.jpg"
    width="2124"
    height="1128"
    alt="Loi de Moore de https://davidwells.io/blog/rise-of-embarrassingly-parallel-serverless-compute"
/>
</div>

## Outils de Benchmarking Continu

Avant de cr√©er Bencher, nous avons cherch√© un outil qui pourrait :

- Suivre les benchmarks √† travers plusieurs langages
- Int√©grer sans probl√®me la sortie des harnais de benchmarking standard des langages
- Extensible pour une sortie de harnais de benchmarking personnalis√©
- Open source et capable d'auto-h√©bergement
- Fonctionner avec plusieurs h√¥tes de CI
- Authentification et autorisation des utilisateurs

Malheureusement, rien qui remplissait tous ces crit√®res n'existait.
Voir [art ant√©rieur](/fr/docs/reference/prior-art/) pour une liste compl√®te des outils de benchmarking existants dont nous nous sommes inspir√©s.

## Benchmarking Continu dans les Grandes Technologies

Des outils comme Bencher ont √©t√© d√©velopp√©s en interne chez
Microsoft, Facebook (maintenant Meta), Apple, Amazon, Netflix, et Google parmi d'innombrables autres.
En tant que titans de l'industrie, ils comprennent l'importance de surveiller la performance pendant le d√©veloppement
et d'int√©grer ces informations dans le processus de d√©veloppement gr√¢ce au CB.
Nous avons construit Bencher pour apporter le benchmarking continu du secret des grandes technologies √† la communaut√© open source.
Pour des liens vers des articles li√©s au benchmarking continu en provenance de grandes technologies, voir [art ant√©rieur](/fr/docs/reference/prior-art/).

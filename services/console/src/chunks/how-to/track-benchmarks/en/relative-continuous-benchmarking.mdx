import TestbedCreate from "../testbed-create.mdx";
import GitCheckoutMain from "../git-checkout-main.mdx";
import RunMainRelative from "../run-main-relative.mdx";
import ThresholdCreateRelative from "../threshold-create-relative.mdx";
import GitCheckoutFeature from "../git-checkout-feature.mdx";
import RunFeatureRelative from "../run-feature-relative.mdx";

## Relative Continuous Benchmarking

Picking up where we left off in the
[Quick Start][quick start] and [Docker Self-Hosted][docker self-hosted] tutorials,
let's add Relative [Continuous Benchmarking][continuous benchmarking] to our `Save Walter White` project.

> 🐰 Make sure you have
> [created an API token and set it as the `BENCHER_API_TOKEN` environment variable][create an api token]
> before continuing on!

[quick start]: /docs/tutorial/quick-start/
[docker self-hosted]: /docs/tutorial/docker/
[continuous benchmarking]: /docs/explanation/continuous-benchmarking/
[create an api token]: /docs/tutorial/quick-start/#create-an-api-token

Relative Continuous Benchmarking runs a side-by-side comparison of two versions of your code.
This can be useful when dealing with noisy CI/CD environments,
where the resources available can be highly variable between runs.
In this example we will be comparing the results from running on the `main` branch
to results from running on a feature branch, aptly named `feature-branch`.
Because every CI environment is a little bit different,
the following example is meant to be more illustrative than practical.
For more specific examples, see [Continuous Benchmarking in GitHub Actions][github actions]
and [Continuous Benchmarking in GitLab CI/CD][gitlab ci/cd].

[github actions]: /docs/how-to/github-actions/
[gitlab ci/cd]: /docs/how-to/gitlab-ci-cd/

First, we need to checkout the `main` branch with `git` in CI:

<GitCheckoutMain />

Then we need to run our benchmarks on the `main` branch in CI:

<RunMainRelative />

1. Use the <code><a href="/docs/explanation/bencher-run/">bencher run</a></code> CLI subcommand
   to run your `main` branch benchmarks.
   See [the `bencher run` CLI subcommand][bencher run] for a full overview.
   (ex: `bencher run`)
2. Set the `--project` option to the Project slug.
   See [the `--project` docs][project option] for more details.
   (ex: `--project save-walter-white-1234abcd`)
3. Set the `--branch` option to the base Branch name.
   See [the `--branch` docs][branch option] for a full overview.
   (ex: `--branch main`)
4. Set the `--start-point-reset` flag to always reset the base Branch.
   This will make sure that all of the benchmark data is from the current CI runner.
   See [the `--start-point-reset` docs][start point reset] for a full overview.
   (ex: `--start-point-reset`)
5. Set the `--testbed` option to the CI runner Testbed name.
   See [the `--testbed` docs][testbed option] for more details.
   (ex: `--testbed ci-runner`)
6. Set the `--adapter` option to [Bencher Metric Format JSON (`json`)][bmf] that is generated by `bencher mock`.
   See [benchmark harness adapters][adapter json] for a full overview.
   (ex: `--adapter json`)
7.  Specify the benchmark command arguments.
    See [benchmark command][command argument] for a full overview.
    (ex: `bencher mock`)

The first time this is command is run in CI,
it will create the `feature-branch` Branch since it does not exist yet.
The new `feature-branch` will _not_ have a start point, existing data, or Thresholds.
On subsequent runs, the old instances of `feature-branch` will be replaced
and a new instance of `feature-branch` will be created without a start point, existing data, or Thresholds.

[bencher run]: /docs/explanation/bencher-run/
[project option]: /docs/explanation/bencher-run/#--project-project
[branch option]: /docs/explanation/branch-selection/#--branch-branch
[start point reset]: /docs/explanation/branch-selection/#--start-point-reset
[testbed option]: /docs/explanation/bencher-run/#--testbed-testbed
[bmf]: /docs/reference/bencher-metric-format/
[adapter json]: /docs/explanation/adapters/#-json
[command argument]: /docs/explanation/bencher-run/#benchmark-command

Next, we need to checkout the `feature-branch` branch with `git` in CI:

<GitCheckoutFeature />

Finally, we are ready to run our `feature-branch` benchmarks in CI:

<RunFeatureRelative />

1. Use the <code><a href="/docs/explanation/bencher-run/">bencher run</a></code> CLI subcommand
   to run your `feature-branch` benchmarks.
   See [the `bencher run` CLI subcommand][bencher run] for a full overview.
   (ex: `bencher run`)
2. Set the `--project` option to the Project slug.
   See [the `--project` docs][project option] for more details.
   (ex: `--project save-walter-white-1234abcd`)
3. Set the `--branch` option to the feature branch Branch name.
   See [the `--branch` docs][branch option] for a full overview.
   (ex: `--branch feature-branch`)
4. Set the Start Point for the `feature-branch` Branch:
   1. Set the `--start-point` option to the feature Branch start point.
   See [the `--start-point` docs][start point] for a full overview.
   (ex: `--start-point main`)
   2. Set the `--start-point-reset` flag to always reset the Branch to the start point.
   This will use only the latest relative benchmark results.
   See [the `--start-point-reset` docs][start point reset] for a full overview.
   (ex: `--start-point-reset`)
5. Set the `--testbed` option to the CI runner Testbed name.
   See [the `--testbed` docs][testbed option] for more details.
   (ex: `--testbed ci-runner`)
6. Set the Threshold for the `feature-branch` Branch, `ci-runner` Testbed, and `latency` Measure:
   1. Set the `--threshold-measure` option to the built-in `latency` Measure that is generated by `bencher mock`.
   See [the `--threshold-measure` docs][threshold measure option] for more details.
   (ex: `--threshold-measure latency`)
   2. Set the `--threshold-test` option to a basic percentage (`percentage`).
   See [the `--threshold-test` docs][threshold test option] for a full overview.
   (ex: `--threshold-test percentage`)
   3. Set the `--threshold-upper-boundary` option to the Upper Boundary of `0.25`.
   See [the `--threshold-upper-boundary` docs][threshold upper boundary] for more details.
   (ex: `--threshold-upper-boundary 0.25`)
   4. Set the `--thresholds-reset` flag so that only the specified Threshold is active.
   See [the `--thresholds-reset` docs][thresholds reset] for a full overview.
   (ex: `--thresholds-reset`)
7. Set the `--err` flag to fail the command if an Alert is generated.
   See [the `--err` docs][alert err] for a full overview.
   (ex: `--err`)
8. Set the `--adapter` option to [Bencher Metric Format JSON (`json`)][bmf] that is generated by `bencher mock`.
   See [benchmark harness adapters][adapter json] for a full overview.
   (ex: `--adapter json`)
9. Specify the benchmark command arguments.
   See [benchmark command][command argument] for a full overview.
   (ex: `bencher mock`)

Every time this command is run in CI,
it is comparing the results from `feature-branch` against only the most recent results from `main`.
The specified Threshold is then used to detect performance regressions.

[start point]: /docs/explanation/branch-selection/#--start-point-branch
[start point reset]: /docs/explanation/branch-selection/#--start-point-reset
[threshold measure option]: /docs/explanation/thresholds/#--threshold-measure-measure
[threshold test option]: /docs/explanation/thresholds/#--threshold-test-test
[threshold upper boundary]: /docs/explanation/thresholds/#--threshold-upper-boundary-boundary
[thresholds reset]: /docs/explanation/thresholds/#--thresholds-reset
[alert err]: /docs/explanation/thresholds/#--err

Most benchmark results are ephemeral.
They disappear as soon as your terminal reaches its scrollback limit.
Some benchmark harnesses let you cache results, but most only do so locally.
Bencher allows you to track your benchmarks from both local and CI runs and compare the results,
while still using [your favorite benchmark harness][adapters].

There are three popular ways to compare benchmark results when [Continuous Benchmarking][continuous benchmarking], that is benchmarking in CI:

- [Statistical Continuous Benchmarking][statistical continuous benchmarking]
  1. Track benchmark results over time to create a baseline
  2. Use this baseline along with [Statistical Thresholds][thresholds] to create a statistical boundary
  3. Compare the new results against this statistical boundary to detect performance regressions
- [Relative Continuous Benchmarking][relative continuous benchmarking]
  1. Run the benchmarks for the current baseline code
  2. Switch over to the new version of the code
  3. Run the benchmarks for the new version of the code
  4. Use [Percentage Thresholds][percentage thresholds] to create a boundary for the baseline code
  5. Compare the new version of the code results against the baseline code results to detect performance regressions
- [Change Point Detection][change point detection]
  1. Occasionally run the benchmarks for new versions of the code
  2. Use a change point detection algorithm to detect performance regressions
  3. Bisect to find the commit that introduced the performance regression

[adapters]: /docs/explanation/adapters/
[continuous benchmarking]: /docs/explanation/continuous-benchmarking/
[thresholds]: /docs/explanation/thresholds/
[percentage thresholds]: /docs/explanation/thresholds/#percentage-thresholds

[statistical continuous benchmarking]: #statistical-continuous-benchmarking
[relative continuous benchmarking]: #relative-continuous-benchmarking
[change point detection]: #change-point-detection
import TestbedCreate from "../testbed-create.mdx";
import ThresholdCreateStatistical from "../threshold-create-statistical.mdx";
import RunMainStatistical from "../run-main-statistical.mdx";
import RunFeatureStatistical from "../run-feature-statistical.mdx";

## Statistical Continuous Benchmarking

Picking up where we left off in the
[Quick Start][quick start] and [Docker Self-Hosted][docker self-hosted] tutorials,
let's add Statistical [Continuous Benchmarking][continuous benchmarking] to our `Save Walter White` project.

> 🐰 Make sure you have
> [created an API token and set it as the `BENCHER_API_TOKEN` environment variable][create an api token]
> before continuing on!

First, we need to create a new Testbed to represent our CI runners, aptly named `ci-runner`.

<TestbedCreate />

1. Use the `bencher testbed create` CLI subcommand.
   See [the `testbed create` docs][testbed create] for more details.
   (ex: `bencher testbed create`)
2. Set the `--name` option to the desired Testbed name.
   (ex: `--name ci-runner`)
3. Specify the project argument as the `Save Walter White` project slug.
   (ex: `save-walter-white-1234abcd`)

Next, we need to create a new [Threshold][thresholds] for our `ci-runner` Testbed:

<ThresholdCreateStatistical />

1. Use the `bencher threshold create` CLI subcommand.
   See [the `threshold create` docs][threshold create] for more details.
   (ex: `bencher threshold create`)
2. Set the `--branch` option to the default `main` Branch.
   (ex: `--branch main`)
3. Set the `--branch` option to the new `ci-runner` Testbed.
   (ex: `--testbed ci-runner`)
4. Set the `--measure` option to the built-in `Latency` Measure that is generated by `bencher mock`.
   See [the definition of Measure][measure] for more details.
   (ex: `--measure Latency`)
5. Set the `--test` option to a `t-test` Threshold.
   See [Thresholds & Alerts][t-test] for a full overview.
   (ex: `--test t-test`)
6. Set the `--upper-boundary` option to an Upper Boundary of `0.95`.
   See [Thresholds & Alerts][t-test upper boundary] for a full overview.
   (ex: `--upper-boundary 0.95`)
7. Specify the project argument as the `Save Walter White` project slug.
   (ex: `save-walter-white-1234abcd`)

Now we are ready to run our benchmarks in CI.
Because every CI environment is a little bit different,
the following example is meant to be more illustrative than practical.
For more specific examples, see [Continuous Benchmarking in GitHub Actions][github actions]
and [Continuous Benchmarking in GitLab CI/CD][gitlab ci/cd].

We need to create and maintain a historical baseline for our `main` branch by benchmarking every change in CI:

<RunMainStatistical />

1. Use the <code><a href="/docs/explanation/bencher-run/">bencher run</a></code> CLI subcommand
   to run your `feature-branch` branch benchmarks.
   See [the `bencher run` CLI subcommand][bencher run] for a full overview.
   (ex: `bencher run`)
2. Set the `--project` option to the Project slug.
   See [the `--project` docs][project option] for more details.
   (ex: `--project save-walter-white-1234abcd`)
3. Set the `--branch` option to the default Branch name.
   See [branch selection][branch selection branch] for a full overview.
   (ex: `--branch main`)
4. Set the `--testbed` option to the Testbed name.
   See [the `--tested` docs][testbed option] for more details.
   (ex: `--testbed ci-runner`)
5. Set the `--adapter` option to the desired benchmark harness adapter.
   See [benchmark harness adapters][adapters] for a full overview.
   (ex: `--adapter json`)
6. Set the `--err` flag to fail the command if an Alert is generated.
   See [Threshold & Alerts][alerts] for a full overview.
   (ex: `--err`)
7. Specify the benchmark command arguments.
   See [benchmark command][command argument] for a full overview.
   (ex: `bencher mock`)

Finally, we are ready to catch performance regressions in CI.
This is how we would track the performance of a new feature branch, named `feature-branch`, in CI:

<RunFeatureStatistical />

1. Use the <code><a href="/docs/explanation/bencher-run/">bencher run</a></code> CLI subcommand
   to run your `feature-branch` branch benchmarks.
   See [the `bencher run` CLI subcommand][bencher run] for a full overview.
   (ex: `bencher run`)
2. Set the `--project` option to the Project slug.
   See [the `--project` docs][project option] for more details.
   (ex: `--project save-walter-white-1234abcd`)
3. Set the `--branch` option to the feature Branch name.
   See [branch selection][branch selection branch] for a full overview.
   (ex: `--branch feature-branch`)
4. Set the `--branch-start-point` option to the feature Branch start point.
   See [branch selection][branch selection start point] for a full overview.
   (ex: `--branch-start-point main`)
5. Set the `--branch-start-point-hash` option to the feature Branch start point `git` hash.
   See [branch selection][branch selection start point hash] for a full overview.
   (ex: `--branch-start-point-hash 32ae...dd8b`)
6. Set the `--branch-reset` flag to always reset the Branch to the start point.
   This will prevent benchmark data drift.
   See [branch selection][branch selection branch reset] for a full overview.
   (ex: `--branch-reset`)
7. Set the `--testbed` option to the Testbed name.
   See [the `--tested` docs][testbed option] for more details.
   (ex: `--testbed ci-runner`)
8. Set the `--adapter` option to the desired benchmark harness adapter.
   See [benchmark harness adapters][adapters] for a full overview.
   (ex: `--adapter json`)
9. Set the `--err` flag to fail the command if an Alert is generated.
   See [Threshold & Alerts][alerts] for a full overview.
   (ex: `--err`)
10. Specify the benchmark command arguments.
   See [benchmark command][command argument] for a full overview.
   (ex: `bencher mock`)

The first time this is command is run in CI,
it will create the `feature-branch` Branch since it does not exist yet.
The new `feature-branch` will use the `main` Branch
at hash `32aea434d751648726097ed3ac760b57107edd8b` as its start point.
This means that `feature-branch` will have a copy of all the data and [Thresholds][thresholds]
from the `main` Branch to compare the results of `bencher mock` against,
for the first and all subsequent runs.

[quick start]: /docs/tutorial/quick-start/
[docker self-hosted]: /docs/tutorial/docker/
[continuous benchmarking]: /docs/explanation/continuous-benchmarking/
[create an api token]: /docs/tutorial/quick-start/#create-an-api-token
[testbed create]: /docs/api/projects/testbeds/#post-v0projectsprojecttestbeds
[thresholds]: /docs/explanation/thresholds/
[threshold create]: /docs/api/projects/thresholds/#post-v0projectsprojectthresholds
[measure]: /docs/explanation/benchmarking/#measures
[t-test]: /docs/explanation/thresholds/#t-test-thresholds
[t-test upper boundary]: /docs/explanation/thresholds/#t-test-threshold-upper-boundary
[github actions]: /docs/how-to/github-actions/
[gitlab ci/cd]: /docs/how-to/gitlab-ci-cd/
[bencher run]: /docs/explanation/bencher-run/
[project option]: /docs/explanation/bencher-run/#--project-project
[branch selection branch]: /docs/explanation/branch-selection/#--branch-branch
[testbed option]: /docs/explanation/bencher-run/#--testbed-testbed
[adapters]: /docs/explanation/adapters/
[alerts]: /docs/explanation/thresholds/#alerts
[command argument]: /docs/explanation/bencher-run/#benchmark-command
[branch selection start point]: /docs/explanation/branch-selection/#--branch-start-point-branch
[branch selection start point hash]: /docs/explanation/branch-selection/#--branch-start-point-hash-hash
[branch selection branch reset]: /docs/explanation/branch-selection/#--branch-reset
Most benchmark results are ephemeral.
They disappear as soon as your terminal reaches its scrollback limit.
Some benchmark harnesses let you cache results, but most only do so locally.
Bencher allows you to track your benchmarks from both local and CI runs and compare against historical results.

The easiest way to track your benchmarks is the <code><a href="/docs/explanation/bencher-run/">bencher run</a></code> CLI subcommand.
It wraps your existing benchmark harness output and generates a Report.
This Report is then sent to the Bencher API server,
where the benchmark harness output is parsed using a [benchmark harness adapter](/docs/explanation/adapters/).
The benchmark harness adapter detects all of the Benchmarks that are present and their corresponding Metrics.
These Benchmarks and Metrics are then saved along with the Report.
If there is a [Threshold](/docs/explanation/thresholds/) set,
then the new Metrics are compared against the historical Metrics for each Benchmark present in the Report.
If a regression is detected, then an Alert will be generated.

From here on out we will refer to your "benchmarks" as "performance regression tests" to avoid any confusion.
